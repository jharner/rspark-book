---
title: "Spark Basics"
author: "Jim Harner"
date: "1/6/2020"
output:
  html_document: default
  html_notebook: default
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 5.1 Sparklyr Basics

The `sparklyr` package is being developed by RStudio. It is undergoing rapid expansion. See [RStudio's sparklyr](https://spark.rstudio.com) for information.

The `sparklyr` R package provides a `dplyr` backend to Spark. Using `sparklyr`, you can: 

* filter and aggregate Spark DataFrames and bring them into R for analysis and visualization;  
* develop workflows using `dplyr` and compatible R packages;  
* write R code to access Spark's machine learning library, [MLlib](http://spark.apache.org/docs/latest/mllib-guide.html);  
* create Spark extensions.  

Using `sparklyr`, connections can be made to local instances or to remote Spark clusters. In our case the connection is to a local connection bundled in the `rstudio` container. 

The `sparklyr` library is loaded in the setup above and a Spark connection is established. The Spark connection `sc` provides a `dplyr` interface to Spark.

### 5.1.1 dplyr

The `dpyr` verbs, e.g., `mutate`, `filter`, can be used on Spark DataFrames. A more complete discussion is given in Section 5.2.

We will use the `flights` data in the `nycflights13` package as an example. If its size becomes an issue, execute each chunk in sequence in notebook mode.
```{r}
library(nycflights13)
str(flights)
```
The `flights` R data frame is a tibble, which allows large data to be displayed. This data frame has the date of departure, the actual departure time, etc. See the package documentation for variable definitions.

The `copy_to` function copies an R `data.frame` to Spark as a Spark table. The resulting object is a `tbl_spark`, which is a `dplyr`-compatible interface to the Spark DataFrame.
```{r}
flights_tbl <- copy_to(sc, nycflights13::flights, "flights")
flights_tbl
src_tbls(sc)
```
By default, the `flights` Spark table is cached in memory (`memory = TRUE`), which speeds up computations, but by default the table is not partitioned (`repartition = 0L`) since we are not running an actual cluster. See the `copy_to` function in the `sparklyr` package for more details.

The Spark connection should be disconnected at the end of a task.
```{r}
spark_disconnect(sc)
```